{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a95f31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning: k-means with Scikit-Learn  \n",
    "\n",
    "What to take away from this lecture:\n",
    "- How to set up a scikit learn method\n",
    "- How k-means works\n",
    "- The importance of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57839d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What will happen: \n",
    "- I'll show you some code\n",
    "- You will have time to experiment in randomly assigned groups of three people\n",
    "- One or two randomly chosen groups get to present their results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476101e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005b85a-5c38-4785-9fb7-52000134d78b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "from sklearn import datasets \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# set plot parameters\n",
    "mpl.rc('axes', labelsize=14)  # axis labels\n",
    "mpl.rc('xtick', labelsize=12)  # axis ticks\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rc('legend', fontsize=16) \n",
    "mpl.rc('legend', title_fontsize=16)\n",
    "mpl.rc('figure', figsize=(16, 8))\n",
    "# mpl.rc('figure', titlesize=20)\n",
    "seed = 1337  # used to get the same result each run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c8597-4bfe-4104-92d2-dbba691614fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup some helper functions for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445d9cc-3963-4078-9092-f885e1050ac8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot data with or without class labels \n",
    "def plot_clusters(X, y=None, title='Generated blobs'):\n",
    "    fig, ax = plt.subplots(figsize=(10,5))  # create figure\n",
    "    colour = 'k' if y is None else y  # get different colours if there is more than one class\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=colour, s=2)  # setup scatter plot\n",
    "#     if y is not None:  \n",
    "#         # produce a legend with the unique colors from the scatter if we have more than one class \n",
    "#         legend = ax.legend(*scatter.legend_elements(),\n",
    "#                            loc=\"lower left\", title=\"Classes\")\n",
    "#         ax.add_artist(legend)\n",
    "    # axis labels\n",
    "    plt.xlabel('$x_1$')  \n",
    "    plt.ylabel('$x_2$', rotation=0) \n",
    "    plt.title(title, fontsize=18)  # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa600e-dd97-4e03-8864-e8cfba2bd000",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot data with decision boundaries\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=3)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='r', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=50, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(kmeans, X, resolution=1000, show_centroids=True,\n",
    "                             show_decision_boundary=True, show_xlabels=True, show_ylabels=True):\n",
    "    \n",
    "    plot_data(X)\n",
    "    if show_decision_boundary:\n",
    "        mins = X.min(axis=0) - 0.1\n",
    "        maxs = X.max(axis=0) + 0.1\n",
    "        xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                             np.linspace(mins[1], maxs[1], resolution))\n",
    "        Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                    cmap=\"PuBuGn\")\n",
    "        plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                    linewidths=1, colors='k')\n",
    "    if show_centroids:\n",
    "        plot_centroids(kmeans.cluster_centers_)\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaa3fc-c64c-42a5-8526-dfaef858c5df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot the clusters on the left side and the decision boundaries on the right side\n",
    "def show_cluster_progress(X: np.array, y: list, num_plots):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    for e,i in enumerate(range(0, num_plots, 2)):\n",
    "        ax1 = fig.add_subplot(3,2, (i+1))\n",
    "        plot_decision_boundaries(y[e], X, show_centroids=True, show_decision_boundary=False, show_xlabels=False, show_ylabels=False)\n",
    "        if e == 0: ax1.set_title('Centroid placing', fontsize=20)\n",
    "        ax2 = fig.add_subplot(3,2, (i+2))\n",
    "        plot_decision_boundaries(y[e], X, show_centroids=False, show_decision_boundary=True, show_xlabels=False, show_ylabels=False)\n",
    "        if e == 0: ax2.set_title('Decision boundaries', fontsize=20)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bc39f-2d4b-42d9-a1d3-6b6c71ab7fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot the clusters on the left side and the decision boundaries on the right side\n",
    "def compare_clustering(X: np.array, y: list, num_plots):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    for e,i in enumerate(range(0, num_plots, 2)):\n",
    "        ax1 = fig.add_subplot(3,2, (i+1))\n",
    "        plot_decision_boundaries(y[e], X, show_centroids=True, show_decision_boundary=True, show_xlabels=False, show_ylabels=False)\n",
    "        ax1.set_title(f'inertia {round(y[e].inertia_, 2)}', fontsize=20) \n",
    "        ax2 = fig.add_subplot(3,2, (i+2))\n",
    "        plot_decision_boundaries(y[e+1], X, show_centroids=True, show_decision_boundary=True, show_xlabels=False, show_ylabels=False)\n",
    "        ax2.set_title(f'inertia {round(y[e+1].inertia_, 2)}', fontsize=20)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c26f5-39ef-4ef0-b1a9-c044a44561ae",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def silhouette_plot(X, kmeans_k):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for k in (3, 4, 5, 6):\n",
    "        plt.subplot(2, 2, k - 2)\n",
    "\n",
    "        y_pred = kmeans_k[k - 1].labels_\n",
    "        silhouette_coefficients = silhouette_samples(X, y_pred)  # computes the Silhouette Coefficient for each sample\n",
    "\n",
    "        padding = len(X) // 30\n",
    "        pos = padding\n",
    "        ticks = []\n",
    "        for i in range(k):\n",
    "            coeffs = silhouette_coefficients[y_pred == i]\n",
    "            coeffs.sort()\n",
    "\n",
    "            color = mpl.cm.Spectral(i / k)\n",
    "            plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ticks.append(pos + len(coeffs) // 2)\n",
    "            pos += len(coeffs) + padding\n",
    "\n",
    "        plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "        plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "        if k in (3, 5):\n",
    "            plt.ylabel(\"Cluster\")\n",
    "\n",
    "        if k in (5, 6):\n",
    "            plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "            plt.xlabel(\"Silhouette Coefficient\")\n",
    "        else:\n",
    "            plt.tick_params(labelbottom=False)\n",
    "\n",
    "        plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "        plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f27c5-7367-4b89-8b86-b67041dde564",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation\n",
    "Create some artificial data using sklearn's make_blob function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662f1e8-b334-418c-93a5-9c6aa746c18f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.3,  1.8],\n",
    "     [-2.8,  3.1],\n",
    "     [-0.5,  1.2]])\n",
    "\n",
    "blob_std = np.array([0.4, 0.3, 0.15, 0.1, 0.2])\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=3000, centers=blob_centers, n_features=2,\n",
    "                  cluster_std=blob_std, random_state=seed)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f030df-cbb7-4bff-97bb-3506197df37e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Take a look at the data\n",
    "\n",
    "The idea of unsupervised learning is to find patterns in the dataset. In the real world, the dataset has no labels, hence it is never quite clear if the result is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07d796-af1d-4a9e-8a9e-37825766e010",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397a07b-856f-43ec-bbd6-029c305a4224",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot the dataset with the given, but in real life unknown labels\n",
    "plot_clusters(X, y, title='Generated blobs with labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b54cd4-55bd-48c9-9c05-ab7407203e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up the k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be26e7f-5251-45cd-8366-e521f53f42e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### setup hyperparameters\n",
    "# predetermine the number of clusters\n",
    "k = 5\n",
    "\n",
    "### setup the k-means scikit learn class\n",
    "kmeans = KMeans(n_clusters=k, \n",
    "                init='random',  # how to set the first clusters \n",
    "                n_init=10,  # number of iterations of which to use the best one\n",
    "                max_iter=50,  # maximum num of iterations, if convergence is not reached prematurely\n",
    "                tol=1e-05,  # tolerance value for the convergence calculation\n",
    "                random_state=seed)  \n",
    "\n",
    "# kmeans.fit(X)  # compute the clustering \n",
    "# y_pred = kmeans.predict(X)  # predict the closest cluster each sample in X belongs to\n",
    "y_pred = kmeans.fit_predict(X)  # combined method. Note that in supervised learning we would use a validation set for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc0aad-dc80-4463-b425-e447b84a0823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the kmeans' prediction\n",
    "plot_clusters(X, y_pred, title='k-means prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f4577-10b5-4c5c-9edc-515313400ff6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# determine the differences between labels and predictions\n",
    "foo = np.copy(y)\n",
    "for a, b in zip(range(5), range(10,15)): \n",
    "    foo[y==a] = b\n",
    "for a, b in zip([3,4,1,2,0], range(10,15)): \n",
    "    foo[foo==b] = a\n",
    "\n",
    "bar = np.equal(foo, y_pred)\n",
    "plot_clusters(X, bar, title='Difference between labels and k-means predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5adc84-3815-4bb5-9b41-411f89bdb3f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's take a closer look at how the algorithm works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17db5d5-f132-400d-a2e1-4cd560eeaca9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans_beginning = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=1, random_state=seed)\n",
    "kmeans_middle = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=10, random_state=seed)\n",
    "kmeans_final = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=50, random_state=seed)\n",
    "\n",
    "kmeans_beginning.fit(X)\n",
    "kmeans_middle.fit(X)\n",
    "kmeans_final.fit(X)\n",
    "\n",
    "kmeans_states = [kmeans_beginning, kmeans_middle, kmeans_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9c971-1c6b-4d1f-896d-9ba7c7618abc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_cluster_progress(X, kmeans_states, num_plots=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c1cb3-6c68-41d2-aac3-aec5b07312b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to measure k-means performance\n",
    "\n",
    "k-means tries to minimise the within-cluster sum of squared errors (SSE), also called **inertia**\n",
    "\n",
    "$ SSE = \\sum \\limits_{i=1}^{n} \\sum \\limits_{j=1}^{k} w^{(i,j)} \\mid\\mid \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{j} \\mid\\mid_{2}^{2} $,\n",
    "\n",
    "where $\\boldsymbol{x}^{i} $ is a datapoint and $\\boldsymbol{\\mu}^{j}$ is the centroid of cluster $j$. If example $\\boldsymbol{x}^{i} $ lies within cluster $j$, then $w^{(i,j)}$ is 1, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7a4a5-db59-4892-91d8-739bafaac6cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Inertia of the barely trained k-means: {kmeans_beginning.inertia_}')\n",
    "print(f'Inertia of the somewhat trained k-means: {kmeans_middle.inertia_}')\n",
    "print(f'Inertia of the properly trained k-means: {kmeans_final.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25636ce-b5f8-4cb3-89d7-9c2d0d022d93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's further analyse k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ef065-057b-441a-842d-634c4b09c49d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. What value should we choose for k?\n",
    "For higher dimensional data, it is usually not possible to directly see how many clusters there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1cc6d-e98f-4950-8902-234ff49a0e36",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def iris_scatter(X_iris, color='k'):\n",
    "    X_iris = PCA(n_components=3).fit_transform(X_iris)\n",
    "    fig = plt.figure(1, figsize=(16,8))\n",
    "    ax = Axes3D(fig, elev=-150, azim=110, auto_add_to_figure=False)\n",
    "    fig.add_axes(ax)\n",
    "    ax.scatter(X_iris[:, 0], X_iris[:, 1], X_iris[:, 2], c=color,\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10221af-0e00-4bd5-bba7-3505f0fbefa4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x_iris = iris.data\n",
    "y_iris = iris.target\n",
    "iris_scatter(x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351e1bc-c758-482c-9e9e-37fbd2d9f34f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris_scatter(x_iris, color=y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830e144-51f3-4c2e-af33-d6a17a35d45b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Plotting the inertia of different cluster numbers $k$\n",
    "\n",
    "Let's compute the inertia of k-means with different cluster numbers and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11167e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_k = [KMeans(n_clusters=k, random_state=seed).fit(X) for k in range(1, 10)]\n",
    "inertias = [km.inertia_ for km in kmeans_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b18c2a-7ffd-48c9-882c-28efdf150277",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(inertias, 'ko-')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('inertia')\n",
    "plt.annotate('elbow?',\n",
    "             xy=(4, inertias[4]),\n",
    "             xytext=(0.5, 0.3),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef1e91-efcc-4f0c-a85b-dc7346bb569b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The *elbow* indicates a good choice for *k*, but it's not necessariliy the correct one - in our case that would be $k = 5$. However, looking at the scatterplot with $k = 4$ shows that it really isn't that bad of a choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821b968-6833-4d3b-9c2d-173bc98dd5b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundaries(kmeans_k[3], X, show_centroids=True, show_decision_boundary=False, show_xlabels=False, show_ylabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c8bd3-afe9-4da9-88f9-746aa60456b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And obviously, the more clusters we choose, the better (lower) the inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384157bf-03af-471e-ba88-ee3092668ba7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundaries(kmeans_k[7], X, show_centroids=True, show_decision_boundary=False, show_xlabels=False, show_ylabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873db75-2ae5-4c03-ad1c-53119e2b6880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Calculating the silhouette score\n",
    "\n",
    "**Silhouette score**\n",
    "\n",
    "The silhouette score determines how tightly grouped the examples in each cluster are. We can calculate the silhouette coefficient for a single datapoint as follows:\n",
    "\n",
    "1. Calculate the **cluster cohesion** $a^{(i)}$, that is the average distance between datapoint $\\boldsymbol{x}^{(i)}$ and all other datapoints in the same cluster. \n",
    "2. Calculate the **cluster separation** $b^{(i)}$, that is the average distance between datapoint $\\boldsymbol{x}^{(i)}$ and all datapoints in the nearest cluster\n",
    "3. Calculate the **silhouette** $s^{(i)}$, that is the difference between cluster cohesion and cluster separation divided by the greater of the two: \n",
    "\n",
    "<div style=\"font-size: 20px\">\n",
    "<center> $s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max\\{a^{(i)}, b^{(i)}\\}}$\n",
    "</div >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b7e01-e475-4661-bd04-8bfb13f57169",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Evaluation of** $s^{(i)}$\n",
    "\n",
    "The silhouette score ranges from -1 to 1, where 1 is ideal. \n",
    "- $s^{(i)} = 0\\colon$ Occurs when cluster cohesion and cluster separation are equal $(b^{(i)} - a^{(i)})$\n",
    "- $s^{(i)} \\to 1\\colon$ Occurs if $b^{(i)} \\gg a^{(i)}$, that is if the clusters are distinctively separated and dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db400f-d851-4447-b7c1-247efd84abe4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average silhouette score \n",
    "\n",
    "One way to visualise the silhouette score is by computing the average for each number of clusters $k$, using scikit-learn's silhouette_score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b19c83-6c97-43bb-8ceb-1c73212abf60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "silhouette_scores = [silhouette_score(X, km.labels_) for km in kmeans_k[1:]]  # there must be more than one cluster, hence [1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d0a45-5dc4-4d0c-aa6a-8997842d334b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"ko-\")\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75427703-88c3-4ced-ba9c-816a145b8bf5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Silhouette plot\n",
    "\n",
    "Another way to visualise the silhouette score is by plotting it for each datapoint of every cluster for different $k$'s, as shown below. The dotted red line is the average silhouette coefficient.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd22a93-624f-4c6d-b4ba-73af94f070f2",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "silhouette_plot(X, kmeans_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de3ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of \"What value should we choose for k?\" \n",
    "\n",
    "Plotting the inertia and silhouette score helps us to choose the correct cluster number $k$, where depending on how the clusters are distributed several cluster numbers seem reasonable (in our case 4 and 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a58a0f-dd10-492a-8068-7034cfc5b09f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. How does the initialisation influence the result?\n",
    "\n",
    "Let's try initialising k-means with different seeds to see for ourselves, starting with the initialisation itself (max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa17df5-5784-40d3-a36a-134ba359740a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "max_iter = 1\n",
    "\n",
    "kmeans_init_1 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+1)\n",
    "kmeans_init_2 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+42)\n",
    "\n",
    "kmeans_init_1.fit(X)\n",
    "kmeans_init_2.fit(X)\n",
    "\n",
    "kmeans_init = [kmeans_init_1, kmeans_init_2]\n",
    "compare_clustering(X, kmeans_init, num_plots=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2344e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now after 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bd227-0f07-4166-80de-39ad329c6cd9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "max_iter = 50\n",
    "\n",
    "kmeans_init_1 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+1)\n",
    "kmeans_init_2 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+42)\n",
    "\n",
    "kmeans_init = [kmeans_init_1.fit(X), kmeans_init_2.fit(X)]\n",
    "\n",
    "compare_clustering(X, kmeans_init, num_plots=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884798da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Allowing for more iterations doesn't make up for a bad initialisation, as the second variant is stuck in a local optimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa7003-6691-4f18-8d2f-104659632167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "max_iter = 10000\n",
    "\n",
    "kmeans_init_1 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+1)\n",
    "kmeans_init_2 = KMeans(n_clusters=k, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+42)\n",
    "\n",
    "kmeans_init = [kmeans_init_1.fit(X), kmeans_init_2.fit(X)]\n",
    "\n",
    "compare_clustering(X, kmeans_init, num_plots=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b2e8e-0790-4c27-ac16-126226fde26f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One option to solve this: Increase the number of initialisations (n_init, defaults to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adaa1f-d975-46ce-84d5-6b02c3a35a49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "max_iter = 20\n",
    "n_init_1 = 1\n",
    "n_init_2 = 10\n",
    "\n",
    "kmeans_init_1 = KMeans(n_clusters=k, init=\"random\", n_init=n_init_1,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+42)  # with only one init\n",
    "kmeans_init_2 = KMeans(n_clusters=k, init=\"random\", n_init=n_init_2,\n",
    "                     algorithm=\"full\", max_iter=max_iter, random_state=seed+42)  # with ten inits\n",
    "\n",
    "kmeans_init = [kmeans_init_1.fit(X), kmeans_init_2.fit(X)]\n",
    "\n",
    "compare_clustering(X, kmeans_init, num_plots=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e659b-4ca0-48da-92af-42446d14ce5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course exercise: What other options are there to improve the algorithm?\n",
    "\n",
    "1. Read the scikit-learn documentation to find out about other k-means hyperparameters, e.g. the initialisation method, and give them a try. How far can you improve the inertia? How high (bad) can you get the inertia?\n",
    "2. How well does k-means perform on the second blob dataset below, which features non-spherical clusters? What is the best inertia you can achieve (with k=3)?\n",
    "3. Browse for another clustering method, implement and test it. What are the best inertias you can achieve for the both datasets? What are the pros and cons in comparison to k-means? What type of clustering algorithm is it (prototype-based, hierarchical, density-based) and why?\n",
    "4. Optional: Be creative! Use other datasets, e.g. the Iris dataset, or use clustering to do something fancy, like image segmentation, or take a preview on supervised learning with nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556ca5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X1, y1 = datasets.make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = datasets.make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "plot_clusters(X, title='Second blob dataset')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:oth] *",
   "language": "python",
   "name": "conda-env-oth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
